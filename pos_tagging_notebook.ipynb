{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ```tagger.py``` : Tagging sentences using the default tagger\n",
    "```tagger.py``` writes part-of-speech tags for sentences using the default tagger (trained on WSJ).\n",
    "\n",
    "*Note* : Since it uses ```default_tagger.pk```, ```wsj_train.txt```, and ```wsj_test.txt``` to optimize.\n",
    "If it does not exist in ```proof-corpus/```, it will make these files.\n",
    "\n",
    "## Input\n",
    "- --files, -f\n",
    "    - tsv files of sentences \n",
    "\n",
    "## Other arguments\n",
    "- --cores, -p\n",
    "    - Number of cores to use. \n",
    "    - Defaults to 4.\n",
    "- --test, -t\n",
    "    - Prints results (instead of writing to file)\n",
    "- --raw, -r\n",
    "    - Do not correct bracket or alias tags.\n",
    "        - ex : (, [, CASE, CITE.\n",
    "\n",
    "## Output\n",
    "- --output, -o\n",
    "    - Path of (a single) txt file of all the input sentences with their tag \n",
    "    - Example output in ```tagged_sents/```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: ignoring input and appending output to 'nohup.out'\n"
     ]
    }
   ],
   "source": [
    "!nohup python3 tagger.py --files ../../2023-01-04/proof-corpus/sent01.tsv --cores 25 --output tagged_sents/tagged_sentences_test.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ```extract_sents.py``` : Create word bins for optimal tagger\n",
    "```extract_sents.py``` creates word bins for optimal tagger experiment. \n",
    "    Specifically, it extracts sentences that begin with words in word_file from tagged sentences.\n",
    "\n",
    "## Input\n",
    "- --file, -f\n",
    "    - Tagged sentences. \n",
    "    - Saved in ```tagged_sents/```.\n",
    "    - Output of ```tagger.py```.\n",
    "\n",
    "- --word_file, -wf\n",
    "    - Path of txt file containing word list to make bins.\n",
    "        - ex : ```optimal_tagger_extra/word_bin_list.txt```)\n",
    "        *OR*\n",
    "- --word, -w\n",
    "    - the word itself as a string (eg. Note)\n",
    "\n",
    "*Note* : The word must be capitalized for both --word and in --word_list\n",
    "\n",
    "## Other arguments\n",
    "- --unique, -u\n",
    "    - Flag to store unique sentences separately.\n",
    "    - Saved in ```word_bins/unique```.\n",
    "    - If this script is being run in order to make ```training/testing_sets``` using ```make_test_train.py```, add the -u flag as ```make_test_train.py``` uses unique sentences (sentences in ```word_bins/unique```) by default.\n",
    "\n",
    "- --cores, -p\n",
    "    - Number of cores to use. \n",
    "    - Defaults to 4.\n",
    "    \n",
    "- --extension, -e\n",
    "    - Custom extension for filename. \n",
    "    - Adds string to the end of filename.\n",
    "\n",
    "## Output\n",
    "- txt files of sentences starting with specified word (in ```word_bins/``` or ```word_bins/unique/```)\n",
    "    - Default :\n",
    "        - Output is saved in ```word_bins/``` or ```word_bins/unique/``` depending on whether the -u flag is used.\n",
    "        - The file name is automatically formatted to be the word (by which we're creating the word bin).\n",
    "\n",
    "            > ```file_name = \"word_bins/\" + word + args.extension + \".txt\"```\n",
    "\n",
    "            > ```file_name_unique = \"word_bins/unique/\" + word + args.extension + \".txt\"```\n",
    "\n",
    "    - --output, -o\n",
    "        - Path of txt file to write sentences to.\n",
    "                            \n",
    "    - --unique_output, -uo\n",
    "        - Path of txt file to write unique sentences to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: ignoring input and appending output to 'nohup.out'\n"
     ]
    }
   ],
   "source": [
    "!nohup python3 extract_sents.py --file tagged_sents/tagged_sentences_test.txt --word_file word_bin_list.txt --cores 20 --unique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ```make_test_train.py``` : Make testing and training bins\n",
    "```make_test_train.py``` makes training and testing sets from word bins for optimal tagger experiment.\n",
    "\n",
    "## Input\n",
    "- --word_list \n",
    "    - Filename of txt file containing the list of words that should be used as the ```training/testing_set```.\n",
    "    - Saved in ```word_lists/```.\n",
    "        - Format : ```word_tag\\n```\n",
    "            - ex : ```Let_VBP\\n```\n",
    "    - Based on this word list, the script will look for the appropriate word bin to use in ```word_bins/unique``` and extract the specified number of sentences per bin.\n",
    "    - Then, the first word will be retagged correctly (based on the tag specified in the word list).\n",
    "    - *Note* : This retagging process is aggresive, and we can only specify one correct tag for each word.\n",
    "    - Words for the training set must be followed by the words for the testing set.\n",
    "    - --num_test_bins (=n) is used to split the word list into training and testing. (bottom n : testing, rest : training)\n",
    "\n",
    "- --train (optional) \n",
    "    - Filename of txt file containing training set.\n",
    "    - Saved in ```training_set/```.\n",
    "    - This is used when we want to create new testing sets for training sets that we made previously, while ensuring that our training and testing sets do not have common sentences. \n",
    "    (Training set will not be created if this argument is used)\n",
    "    - It is recommended to use this option whenever training and testing sets will be made using the same word bins. \n",
    "    (ie do not try to make the training and testing sets in one go (without using --train) if they are going to be created from the same bins.)\n",
    "    \n",
    "## Other arguments\n",
    "- --num_train_sents, -ntr\n",
    "    - Number of training sentences per bin.\n",
    "    - Defaults to 5.\n",
    "\n",
    "- --num_test_bins, -nte\n",
    "    - Number of testing word bins.\n",
    "    - Defaults to 1.\n",
    "\n",
    "- --train_extension, -tr_e\n",
    "    - Custom extension for training set filename. \n",
    "    - Adds string to the end of filename.\n",
    "\n",
    "- --test_extension, -te_e\n",
    "    - Custom extension for testing set filename. \n",
    "    - Adds string to the end of filename.\n",
    "\n",
    "## Output\n",
    "- txt files of sentences starting with specified word (in ```word_bins/``` or ```word_bins/unique/```)\n",
    "    - Output is saved in ```word_bins/``` or ```word_bins/unique/``` depending on whether the -u flag is used.\n",
    "    - The filename is automatically formatted to be the word (by which we're creating the word bin).\n",
    "\n",
    "        > ```file_name = \"word_bins/\" + word + args.extension + \".txt\"```\n",
    "\n",
    "        > ```file_name_unique = \"word_bins/unique/\" + word + args.extension + \".txt\"```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: ignoring input and appending output to 'nohup.out'\n"
     ]
    }
   ],
   "source": [
    "!nohup python3 make_test_train.py --num_train_sents 5 --num_test_bins 5 --train_extension test --test_extension test --word_list word_lists/nnp_verb_list_test.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: ignoring input and appending output to 'nohup.out'\n"
     ]
    }
   ],
   "source": [
    "# Make new testing bin based on training bin\n",
    "!nohup python3 make_test_train.py --train training_set/test.txt --num_test_bins 1 --test_extension test_2 --word_list word_lists/nnp_verb_list_partition.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ```main_experiment.py``` : Run main experiment\n",
    "\n",
    "```main_experiment.py``` finds the optimal tagger that tags imperative verbs accurately.\n",
    "\n",
    "## Input\n",
    "- --training_set\n",
    "    - Training set (txt file in ```training_set/```)\n",
    "- --testing_set\n",
    "    - Testing set (txt file in ```testing_set/```)\n",
    "\n",
    "*Note* : Both training and testing sets are \"correctly\" tagged.\n",
    "    They can be outputs of ```make_test_train.py``` but note that ```make_test_train``` only \"corrects\" the first word, according to the word list.\n",
    "  \n",
    "## Other arguments\n",
    "- --train, -tr\n",
    "    - txt file to read training set.\n",
    "\n",
    "- --test, -te\n",
    "    - txt file to read testing set.\n",
    "\n",
    "- --cores, -p\n",
    "    - Number of cores to use. \n",
    "    - Defaults to 4.\n",
    "\n",
    "- --extension, -e\n",
    "    - Custom extension for filename. \n",
    "    - Adds string to the end of filename.\n",
    "\n",
    "- --num_trials, -nt\n",
    "    - Number of trials of experiment.\n",
    "    - Defaults to 10.\n",
    "\n",
    "- --train_num_lis, -tnl\n",
    "    - Number of sentences to train on (list in string format).\n",
    "    - Defaults to 5,10.\n",
    "    \n",
    "- --iter_num_list, -inl\n",
    "    - Number of iterations (list in string format).\n",
    "    - Defaults to 5,10.\n",
    "\n",
    "- --wsj_test, -wt\n",
    "    - Tests on WSJ testing corpus.\n",
    "\n",
    "- --print_mislabels, -m\n",
    "    - Prints non-VB tags.\n",
    "\n",
    "- --dump, -d\n",
    "    - Dumps trained tagger.\n",
    "\n",
    "- --tag_n, -tn\n",
    "    - Number of words to tag in each sentence (first n).\n",
    "    - Defaults to None.\n",
    "\n",
    "- --default_results, -dr\n",
    "    - Writes default results to file.\n",
    "\n",
    "## Output\n",
    "- txt file of results.\n",
    "    - Saved in ```experiments/```.\n",
    "- Pickled taggers, depending on flags used.\n",
    "    - Saved in ```tagger/```.\n",
    "\n",
    "The filename is formatted automatically depending on the parameters\n",
    "> ```output_test = \"experiments/experiment_\" + str(num_train_sent) + \"sents_\" + str(nr_iter) + \"iters_\" + extension + \".txt\"```\n",
    "\n",
    "If the -d flag is added, the taggers are dumped for future use (use ```dumped_main_experiment.py```)\n",
    "The filename is formatted automatically (similar to the results, but with the trial ID added at the end)\n",
    ">  ```output_dump = \"tagger/\" + str(num_train_sent) + \"sents_\" + str(nr_iter) + \"iters_\" + extension + \"_\" + str(trial_id) + \".pk\"```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: ignoring input and appending output to 'nohup.out'\n"
     ]
    }
   ],
   "source": [
    "!nohup python3 main_experiment.py --train training_set/test.txt --test testing_set/test.txt --cores 30 --num_trials 5 --train_num_list 1,5 --iter_num_list 5 --extension test --wsj_test --dump --default_results "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ```read_experiment_results.py``` : Read experiment results\n",
    "\n",
    "```read_experiment_results.py``` summarizes results from ```main_experiment.py``` or ```dumped_main_experiment.py``` by taking averages and standard deviations from multiple trials.\n",
    "\n",
    "## Input\n",
    "- --files, -f\n",
    "    - txt files to read experiment results from.\n",
    "    - Files should be outputs of ```main_experiment.py``` or ```dumped_main_experiment.py```.\n",
    "\n",
    "## Output\n",
    "- --output, -o \n",
    "    - Path of txt file of summarized results.\n",
    "\n",
    "## How to read results\n",
    "- Takes the results of n trials of an experiment (done under the same condition) in one txt file, and writes the average and standard deviation for each metric.\n",
    "- Format :\n",
    "    > ```file_name\\taverage_of_first_metric,standard_deviation_of_first_metric\\taverage_of_second_metric,standard_deviation_of_second_metric\\t...\\n``\n",
    "\n",
    "- In the example the following metrics are reported, separated by ```\\t```.\n",
    "    - Accuracy\n",
    "    - Number of VBs mislabeled as NNP\n",
    "    - Number of VBGs mislabeled as NNP\n",
    "    - Number of VBs mislabeled as NN\n",
    "    - Number of NNs mislabeled as JJ\n",
    "    - Number of NNs mislabeled as VB\n",
    "    - Number of NNSs mislabeled as VBZ\n",
    "    - Number of JJ mislabeled as NN\n",
    "    - Number of RB mislabeled as NN\n",
    "    - Number of VBs mislabeled\n",
    "    - Number of mislabellings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment_1sents_5iters_test-wsj\n",
      "[['0.97008534755474', '4', '4', '66', '164', '17', '32', '175', '22', '147', '3130'], ['0.9698368552341085', '5', '4', '70', '166', '16', '32', '171', '22', '156', '3156'], ['0.9698655274249506', '5', '4', '64', '175', '14', '31', '169', '23', '149', '3153'], ['0.9700375605700031', '3', '4', '64', '157', '12', '30', '175', '23', '146', '3135'], ['0.9703242824784242', '4', '4', '61', '164', '16', '24', '175', '25', '147', '3105']]\n",
      "experiment_1sents_5iters_test\n",
      "[['0.9461538461538461', '10', '0', '0', '3', '0', '0', '3', '0', '23', '42'], ['0.9564102564102565', '10', '0', '0', '7', '0', '1', '1', '0', '17', '34'], ['0.958974358974359', '9', '0', '0', '2', '0', '0', '2', '0', '16', '32'], ['0.9525641025641025', '12', '0', '0', '4', '0', '0', '3', '0', '20', '37'], ['0.9346153846153846', '16', '0', '0', '2', '0', '0', '4', '0', '30', '51']]\n",
      "experiment_5sents_5iters_test-wsj\n",
      "[['0.970372069463161', '4', '4', '63', '166', '14', '33', '177', '24', '141', '3100'], ['0.9704676434326347', '5', '4', '62', '169', '14', '29', '164', '22', '146', '3090'], ['0.9700280031730558', '3', '4', '62', '165', '13', '37', '168', '22', '142', '3136'], ['0.9701522493333715', '4', '4', '67', '163', '13', '30', '165', '25', '146', '3123'], ['0.9703338398753716', '3', '4', '68', '166', '16', '38', '173', '26', '145', '3104']]\n",
      "experiment_5sents_5iters_test\n",
      "[['0.9576923076923077', '6', '0', '1', '2', '0', '0', '3', '0', '14', '33'], ['0.9615384615384616', '6', '0', '0', '2', '0', '0', '6', '0', '12', '30'], ['0.9512820512820512', '4', '0', '1', '2', '0', '0', '5', '0', '11', '38'], ['0.9576923076923077', '5', '0', '0', '4', '0', '0', '3', '0', '11', '33'], ['0.9487179487179487', '6', '0', '1', '6', '0', '0', '4', '0', '12', '40']]\n",
      "experiment_default_tagger_test\n",
      "[['0.9320512820512821', '27', '0', '1', '0', '0', '0', '0', '0', '44', '53']]\n"
     ]
    }
   ],
   "source": [
    "!python3 read_experiment_results.py --files experiments/*test* --output experiments/summary_test.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ```dumped_main_experiment.py``` : Run dumped experiment\n",
    "\n",
    "```dumped_main_experiment.py``` runs the main experiment on dumped taggers and return accuracy, # of mislabelled tags etc. (Refer to ```main_experiment.py``` for more detailed information.)\n",
    "\n",
    "\n",
    "## Input\n",
    "-  --tagger\n",
    "    - Dumped taggers. \n",
    "    - Saved in ```tagger/```.\n",
    "    - Dumped using ```main_experiment.py```.\n",
    "- --test\n",
    "    - Testing set.\n",
    "    - Saved in ```testing_set/```. - Made using ```make_test_train.py```.\n",
    "- --tag_n\n",
    "    - Tags only first n words in each sentence.\n",
    "    - Defaults to None, which tags all words.\n",
    "\n",
    "*Note* : Both training and testing sets are \"correctly\" tagged.\n",
    "    They can be outputs of ```make_test_train.py``` but note that ```make_test_train``` only \"corrects\" the first word, according to the word list.\n",
    "  \n",
    "## Other arguments\n",
    "- --test, -te\n",
    "    - txt file to read testing set from.\n",
    "    \n",
    "- --extension, -e\n",
    "    - Custom extension for filename. \n",
    "    - Adds string to the end of filename.\n",
    "    \n",
    "- --cores, -p\n",
    "    - Number of cores to use. \n",
    "    - Defaults to 4.\n",
    "\n",
    "- --wsj_test, -wt\n",
    "    - Tests on WSJ testing corpus.\n",
    "\n",
    "- --print_mislabels, -m\n",
    "    - Output non-VB tags.\n",
    "\n",
    "- --tag_n, -tn\n",
    "    - Number of words to tag in each sentence (first n).\n",
    "    - Defaults to None.\n",
    "\n",
    "## Output\n",
    "- txt file of results.\n",
    "    - Saved in ```experiments/```.\n",
    "    - The filename is formatted automatically depending on the parameters.\n",
    "        > ```output_test = \"experiments/experiment_\" + str(num_train_sent) + \"sents_\" + str(nr_iter) + \"iters_\" + extension + \".txt\"```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: ignoring input and appending output to 'nohup.out'\n"
     ]
    }
   ],
   "source": [
    "!nohup python3 dumped_main_experiment.py --tagger tagger/5sents_5iters_test_trial1.pk --test testing_set/refer_handtagged.txt --extension refer_test -p 25 --tag_n 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ```find_disagreeing_sents.py``` : Finds sentences with \"disagreeing\" first word tags.\n",
    "\n",
    "```find_disagreeing_sents.py``` finds sentences with first words such that the best taggers agree on the tag (best tag), the worst taggers agree on the tag (worst tag), and best tag and worst tag are not the same.\n",
    "\n",
    "## Input\n",
    "- --file, -f\n",
    "    - txt files of tagged sentences to find disagreeing sentences from.\n",
    "\n",
    "- --best_tagger, -b\n",
    "    - IDs of best taggers, split by commas.\n",
    "    - Defaults to 41,9.\n",
    "\n",
    "- --worst_tagger, -w\n",
    "    - IDs of worst taggers, split by commas.\n",
    "    - Defaults to 38,22.\n",
    "\n",
    "*Note* : Taggers are loaded from :\n",
    "> ```TAGGER_PATH = \"tagger/7_5/5sents_5iters_7_5_trial\"```\n",
    "\n",
    "  \n",
    "## Other arguments\n",
    "- --use_default, -d\n",
    "    - Uses default tagger.\n",
    "\n",
    "- --write_tags, -t\n",
    "    - Tag sentences using every tagger (best and worst) and write the tags out.\n",
    "    - Filename is formatted as follows :\n",
    "        > ```\"trial\" + all_taggers_ids[ind] + \".txt\"```\n",
    "\n",
    "## Output\n",
    "- --output, -o\n",
    "    - Path of txt file to write sentences to. \n",
    "    - If unspecified, results are printed.\n",
    "\n",
    "## How to look at the output\n",
    ">```best_tag_of_first_word\\tworst_tag_of_first_word\\tsent```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 find_disagreeing_sents.py -f testing_set/4verbs_handtagged.txt -o disagreeing_sents_7_8.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ```compare_tagger_weights.py``` : Compare tagger weights\n",
    "\n",
    "```compare_tagger_weights.py``` compares weights of trained taggers using output of ```find_disagreeing_sents.py```.\n",
    "\n",
    "## Input\n",
    "- --file, -f\n",
    "    - txt file of sentences.\n",
    "    - In ```disagreeing_sents/```, output of ```disagreeing_sents.py```.\n",
    "\n",
    "- --best_tagger, -b\n",
    "    - IDs of best taggers, split by commas.\n",
    "    - Defaults to 41,9.\n",
    "\n",
    "- --worst_tagger, -w\n",
    "    - IDs of worst taggers, split by commas.\n",
    "    - Defaults to 38,22.\n",
    "\n",
    "*Note* : Taggers are loaded from :\n",
    "> ```TAGGER_PATH = \"tagger/7_5/5sents_5iters_7_5_trial\"```\n",
    "\n",
    "  \n",
    "## Other arguments\n",
    "- --use_default, -d\n",
    "    - Uses default tagger.\n",
    "\n",
    "## Output\n",
    "- --output, -o\n",
    "    - Path of txt file to write to. \n",
    "    - If unspecified, results are printed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "!python3 compare_tagger_weights.py --file disagreeing_sents/disagreeing_sents_7_8.txt --output compare_weights_test.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c0a841c08f84e217821e8f89b083a7daef6824e43204e31278c3f445ff1560d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
