{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ```tagger.py``` : Tagging sentences using the default tagger\n",
    "```tagger.py``` writes part-of-speech tags for sentences using the default tagger (trained on WSJ).\n",
    "\n",
    "*Note* : Since it uses ```default_tagger.pk```, ```wsj_train.txt```, and ```wsj_test.txt``` to optimize.\n",
    "If it does not exist in ```proof-corpus/```, it will make these files.\n",
    "\n",
    "## Input\n",
    "- --files, -f\n",
    "    - tsv files of sentences \n",
    "\n",
    "## Other arguments\n",
    "- --cores, -p\n",
    "    - Number of cores to use. \n",
    "    - Defaults to 4.\n",
    "- --test, -t\n",
    "    - Prints results (instead of writing to file)\n",
    "- --raw, -r\n",
    "    - Do not correct bracket or alias tags.\n",
    "        - ex : (, [, CASE, CITE.\n",
    "\n",
    "## Output\n",
    "- --output, -o\n",
    "    - (a single) txt file of all the input sentences with their tag \n",
    "    - Example output in ```tagged_sents/```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: ignoring input and appending output to 'nohup.out'\n"
     ]
    }
   ],
   "source": [
    "!nohup python3 tagger.py --files ../../2023-01-04/proof-corpus/sent01.tsv --cores 25 --output tagged_sents/tagged_sentences_test.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ```extract_sents.py``` : Create word bins for optimal tagger\n",
    "```extract_sents.py``` creates word bins for optimal tagger experiment. \n",
    "    Specifically, it extracts sentences that begin with words in word_file from tagged sentences.\n",
    "\n",
    "## Input\n",
    "- --file, -f\n",
    "    - tagged sentences (in ```tagged_sents/```, output of ```tagger.py```)\n",
    "- --word_file, -wf\n",
    "    - txt file containing word list to make bins (eg. ```optimal_tagger_extra/word_bin_list.txt```)\n",
    "        or \n",
    "- --word, -w\n",
    "    - the word itself as a string (eg. Note)\n",
    "\n",
    "*Note* : The word must be capitalized for both --word and in --word_list\n",
    "\n",
    "## Other arguments\n",
    "- --unique, -u\n",
    "    - Flag to store unique sentences separately, in ```word_bins/unique```.\n",
    "    - If this script is being run in order to make ```training/testing_sets``` using ```make_test_train.py```, add the -u flag as make_test_train.py uses unique sentences (sentences in ```word_bins/unique```) by default.\n",
    "\n",
    "- --cores, -p\n",
    "    - Number of cores to use. \n",
    "    - Defaults to 4.\n",
    "    \n",
    "- --extension, -e\n",
    "    - Custom extension for filename. Adds string to the end of file name.\n",
    "\n",
    "## Output\n",
    "- txt files of sentences starting with specified word (in ```word_bins/``` or ```word_bins/unique/```)\n",
    "    - Default\n",
    "        - Output is saved in ```word_bins/``` or ```word_bins/unique/``` depending on whether the -u flag is used.\n",
    "        - The file name is automatically formatted to be the word (by which we're creating the word bin).\n",
    "\n",
    "            > ```file_name = \"word_bins/\" + word + args.extension + \".txt\"```\n",
    "\n",
    "            > ```file_name_unique = \"word_bins/unique/\" + word + args.extension + \".txt\"```\n",
    "\n",
    "    - --output, -o\n",
    "        - path of txt file to write sentences to.\n",
    "                            \n",
    "    - --unique_output, -uo\n",
    "        - path of txt file to write unique sentences to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: ignoring input and appending output to 'nohup.out'\n"
     ]
    }
   ],
   "source": [
    "!nohup python3 extract_sents.py --file tagged_sents/tagged_sentences_test.txt --word_file word_bin_list.txt --cores 20 --unique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ```make_test_train.py``` : Make testing and training bins\n",
    "```make_test_train.py``` makes training and testing sets from word bins for optimal tagger experiment.\n",
    "\n",
    "## Input\n",
    "- --word_list \n",
    "    - txt file containing the list of words that should be used as the ```training/testing_set``` (in ```word_lists/```)\n",
    "        - Format : ```word_tag\\n```\n",
    "            - ex : ```Let_VBP\\n```\n",
    "            \n",
    "    - Based on this word list, the script will look for the appropriate word bin to use in ```word_bins/unique``` and extract the specified number of sentences per bin.\n",
    "    - Then, the first word will be retagged correctly (based on the tag specified in the word list).\n",
    "    - *Note* : This retagging process is aggresive, and we can only specify one correct tag for each word.\n",
    "                  \n",
    "    - Words for the training set must be followed by the words for the testing set.\n",
    "\n",
    "    - --num_test_bins (=n) is used to split the word list into training and testing. (bottom n : testing, rest : training)\n",
    "\n",
    "- --train (optional) \n",
    "    - txt file containing training set (in ```training_set/```)\n",
    "    - This is used when we want to create new testing sets for training sets that we made previously, while ensuring that our training and testing sets do not have common sentences. \n",
    "    (Training set will not be created if this argument is used)\n",
    "    - It is recommended to use this option whenever training and testing sets will be made using the same word bins. \n",
    "    (ie do not try to make the training and testing sets in one go (without using --train) if they are going to be created from the same bins)\n",
    "    \n",
    "## Other arguments\n",
    "- --num_train_sents, -ntr\n",
    "    - Number of training sentences per bin.\n",
    "    - Defaults to 5.\n",
    "\n",
    "- --num_test_bins, -nte\n",
    "    - Number of testing word bins.\n",
    "    - Defaults to 1.\n",
    "\n",
    "- --train_extension, -tr_e\n",
    "    - training file extension.\n",
    "\n",
    "    parser.add_argument(\"--test_extension\", \"-te_e\",\n",
    "                            help=\"testing file extension\")\n",
    "\n",
    "\n",
    "## Output\n",
    "- txt files of sentences starting with specified word (in ```word_bins/``` or ```word_bins/unique/```)\n",
    "    - Output is saved in ```word_bins/``` or ```word_bins/unique/``` depending on whether the -u flag is used.\n",
    "    - The file name is automatically formatted to be the word (by which we're creating the word bin).\n",
    "\n",
    "        > ```file_name = \"word_bins/\" + word + args.extension + \".txt\"```\n",
    "\n",
    "        > ```file_name_unique = \"word_bins/unique/\" + word + args.extension + \".txt\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: ignoring input and appending output to 'nohup.out'\n"
     ]
    }
   ],
   "source": [
    "# Make testing and training bins\n",
    "!nohup python3 make_test_train.py --num_train_sents 5 --num_test_bins 5 --train_extension test --test_extension test --word_list word_lists/nnp_verb_list_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: ignoring input and appending output to 'nohup.out'\n"
     ]
    }
   ],
   "source": [
    "# Run main experiment\n",
    "!nohup python3 main_experiment.py --train training_set/test.txt --test testing_set/test.txt --cores 30 --num_trials 5 --train_num_list 1,5 --iter_num_list 5 --extension test --wsj_test --dump --default_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment_1sents_5iters_test-wsj\n",
      "[['0.97008534755474', '4', '4', '66', '164', '17', '32', '175', '22', '147', '3130'], ['0.9698368552341085', '5', '4', '70', '166', '16', '32', '171', '22', '156', '3156'], ['0.9698655274249506', '5', '4', '64', '175', '14', '31', '169', '23', '149', '3153'], ['0.9700375605700031', '3', '4', '64', '157', '12', '30', '175', '23', '146', '3135'], ['0.9703242824784242', '4', '4', '61', '164', '16', '24', '175', '25', '147', '3105']]\n",
      "experiment_1sents_5iters_test\n",
      "[['0.9461538461538461', '10', '0', '0', '3', '0', '0', '3', '0', '23', '42'], ['0.9564102564102565', '10', '0', '0', '7', '0', '1', '1', '0', '17', '34'], ['0.958974358974359', '9', '0', '0', '2', '0', '0', '2', '0', '16', '32'], ['0.9525641025641025', '12', '0', '0', '4', '0', '0', '3', '0', '20', '37'], ['0.9346153846153846', '16', '0', '0', '2', '0', '0', '4', '0', '30', '51']]\n",
      "experiment_5sents_5iters_test-wsj\n",
      "[['0.970372069463161', '4', '4', '63', '166', '14', '33', '177', '24', '141', '3100'], ['0.9704676434326347', '5', '4', '62', '169', '14', '29', '164', '22', '146', '3090'], ['0.9700280031730558', '3', '4', '62', '165', '13', '37', '168', '22', '142', '3136'], ['0.9701522493333715', '4', '4', '67', '163', '13', '30', '165', '25', '146', '3123'], ['0.9703338398753716', '3', '4', '68', '166', '16', '38', '173', '26', '145', '3104']]\n",
      "experiment_5sents_5iters_test\n",
      "[['0.9576923076923077', '6', '0', '1', '2', '0', '0', '3', '0', '14', '33'], ['0.9615384615384616', '6', '0', '0', '2', '0', '0', '6', '0', '12', '30'], ['0.9512820512820512', '4', '0', '1', '2', '0', '0', '5', '0', '11', '38'], ['0.9576923076923077', '5', '0', '0', '4', '0', '0', '3', '0', '11', '33'], ['0.9487179487179487', '6', '0', '1', '6', '0', '0', '4', '0', '12', '40']]\n",
      "experiment_default_tagger_test\n",
      "[['0.9320512820512821', '27', '0', '1', '0', '0', '0', '0', '0', '44', '53']]\n"
     ]
    }
   ],
   "source": [
    "# Read experiment results\n",
    "!python3 read_experiment_results.py --files experiments/*test* --output experiments/summary_test.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: ignoring input and appending output to 'nohup.out'\n"
     ]
    }
   ],
   "source": [
    "# Make new testing bin based on training bin\n",
    "!nohup python3 make_test_train.py --train training_set/test.txt --num_test_bins 1 --test_extension test_2 --word_list word_lists/nnp_verb_list_partition.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: ignoring input and appending output to 'nohup.out'\n"
     ]
    }
   ],
   "source": [
    "# Run dumped experiment\n",
    "!nohup python3 dumped_main_experiment.py --tagger tagger/5sents_5iters_test_trial1.pk --test testing_set/refer_handtagged.txt --extension refer_test -p 25 --tag_n 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "# Compare tagger weights in 7_5/5sents_5iters_7_5_trial\n",
    "!python3 compare_tagger_weights.py --file disagreeing_sents/disagreeing_sents_7_8.txt --output compare_weights_test.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c0a841c08f84e217821e8f89b083a7daef6824e43204e31278c3f445ff1560d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
